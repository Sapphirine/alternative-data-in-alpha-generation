{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"categorial_multi_hourly.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"accelerator":"TPU"},"cells":[{"cell_type":"code","metadata":{"colab_type":"code","id":"qKIsV4IekZx_","colab":{}},"source":["def load_data():\n","  dataset=pd.read_csv(\"dataset_search_sentiment.csv\")\n","  dataset.dropna(inplace=True)\n","  dataset['Open']=pd.to_numeric(dataset['Open'],downcast='float')\n","  dataset['High']=pd.to_numeric(dataset['High'],downcast='float')\n","  dataset['Low']=pd.to_numeric(dataset['Low'],downcast='float')\n","  dataset['Weighted Price']=pd.to_numeric(dataset['Weighted Price'],downcast='float')\n","  #dataset.drop(columns='Timestamp',inplace=True)\n","  dataset.drop(columns='Unnamed: 0',inplace=True)\n","  dataset.drop(columns='datetime',inplace=True)\n","  num_steps=5\n","  dataset_multi = series_to_supervised(dataset, num_steps, 1)\n","  \n","  pd.set_option('display.max_columns', None)\n","  pd.set_option('display.width', None)\n","  pd.set_option('display.max_colwidth', -1)\n","\n","  dataset_multi_np=dataset_multi.values\n","  nn_features=dataset_multi_np[:,0:dataset_multi_np.shape[1]-dataset.shape[1]]\n","  nn_output=dataset_multi_np[:,dataset_multi_np.shape[1]-2*dataset.shape[1]:]\n","\n","  nn_labels=np.divide(nn_output[:,dataset.shape[1]+4],nn_output[:,4])>1\n","  return nn_features,nn_labels"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7maU3A6glIE4","colab_type":"code","colab":{}},"source":["dataset=pd.read_csv(\"dataset_search_sentiment.csv\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JC6JXY4PlKmN","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":810},"outputId":"86cc5b21-c569-429e-d0c4-4df3db8fd597","executionInfo":{"status":"ok","timestamp":1587067269598,"user_tz":240,"elapsed":281,"user":{"displayName":"Ziyun Chen","photoUrl":"","userId":"11569191328117568817"}}},"source":["dataset"],"execution_count":54,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>Open</th>\n","      <th>High</th>\n","      <th>Low</th>\n","      <th>Weighted Price</th>\n","      <th>price</th>\n","      <th>ma7</th>\n","      <th>ma21</th>\n","      <th>26ema</th>\n","      <th>12ema</th>\n","      <th>MACD</th>\n","      <th>20sd</th>\n","      <th>upper_band</th>\n","      <th>lower_band</th>\n","      <th>ema</th>\n","      <th>momentum_1</th>\n","      <th>momentum_2</th>\n","      <th>momentum_3</th>\n","      <th>momentum_4</th>\n","      <th>momentum_5</th>\n","      <th>momentum_1 return</th>\n","      <th>momentum_2 return</th>\n","      <th>momentum_3 return</th>\n","      <th>momentum_4 return</th>\n","      <th>momentum_5 return</th>\n","      <th>datetime</th>\n","      <th>bitcoin</th>\n","      <th>Hourly Polarity Mean</th>\n","      <th>Hourly Polarity Weighted Mean 1</th>\n","      <th>Hourly Polarity Weighted Mean 1_2</th>\n","      <th>Hourly Polarity Weighted Mean 1_4</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2020-02-01 00:00:00</td>\n","      <td>9337.1</td>\n","      <td>9376.3</td>\n","      <td>9336.0</td>\n","      <td>9355.76</td>\n","      <td>9372.0</td>\n","      <td>9334.100000</td>\n","      <td>9319.061905</td>\n","      <td>9338.414300</td>\n","      <td>9331.897840</td>\n","      <td>-6.516460</td>\n","      <td>39.768078</td>\n","      <td>9398.598060</td>\n","      <td>9239.525749</td>\n","      <td>9364.617840</td>\n","      <td>9339.3</td>\n","      <td>9390.0</td>\n","      <td>9350.5</td>\n","      <td>9303.4</td>\n","      <td>9284.6</td>\n","      <td>1.003501</td>\n","      <td>0.998083</td>\n","      <td>1.002299</td>\n","      <td>1.007374</td>\n","      <td>1.009413</td>\n","      <td>2020-02-01 00:00:00</td>\n","      <td>73.000000</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2020-02-01 01:00:00</td>\n","      <td>9372.1</td>\n","      <td>9418.0</td>\n","      <td>9372.0</td>\n","      <td>9396.57</td>\n","      <td>9417.9</td>\n","      <td>9351.100000</td>\n","      <td>9320.085714</td>\n","      <td>9344.302130</td>\n","      <td>9345.128942</td>\n","      <td>0.826812</td>\n","      <td>46.991843</td>\n","      <td>9414.069401</td>\n","      <td>9226.102028</td>\n","      <td>9400.139280</td>\n","      <td>9372.0</td>\n","      <td>9339.3</td>\n","      <td>9390.0</td>\n","      <td>9350.5</td>\n","      <td>9303.4</td>\n","      <td>1.004898</td>\n","      <td>1.008416</td>\n","      <td>1.002971</td>\n","      <td>1.007208</td>\n","      <td>1.012307</td>\n","      <td>2020-02-01 01:00:00</td>\n","      <td>67.000000</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2020-02-01 02:00:00</td>\n","      <td>9418.0</td>\n","      <td>9453.9</td>\n","      <td>9396.3</td>\n","      <td>9427.90</td>\n","      <td>9425.9</td>\n","      <td>9371.285714</td>\n","      <td>9324.890476</td>\n","      <td>9350.346416</td>\n","      <td>9357.555258</td>\n","      <td>7.208842</td>\n","      <td>43.912695</td>\n","      <td>9412.715866</td>\n","      <td>9237.065087</td>\n","      <td>9417.313093</td>\n","      <td>9417.9</td>\n","      <td>9372.0</td>\n","      <td>9339.3</td>\n","      <td>9390.0</td>\n","      <td>9350.5</td>\n","      <td>1.000849</td>\n","      <td>1.005751</td>\n","      <td>1.009273</td>\n","      <td>1.003823</td>\n","      <td>1.008064</td>\n","      <td>2020-02-01 02:00:00</td>\n","      <td>66.000000</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2020-02-01 03:00:00</td>\n","      <td>9437.7</td>\n","      <td>9447.7</td>\n","      <td>9395.8</td>\n","      <td>9412.27</td>\n","      <td>9417.6</td>\n","      <td>9387.600000</td>\n","      <td>9328.014286</td>\n","      <td>9355.328163</td>\n","      <td>9366.792911</td>\n","      <td>11.464748</td>\n","      <td>34.745263</td>\n","      <td>9397.504813</td>\n","      <td>9258.523759</td>\n","      <td>9417.504364</td>\n","      <td>9425.9</td>\n","      <td>9417.9</td>\n","      <td>9372.0</td>\n","      <td>9339.3</td>\n","      <td>9390.0</td>\n","      <td>0.999119</td>\n","      <td>0.999968</td>\n","      <td>1.004866</td>\n","      <td>1.008384</td>\n","      <td>1.002939</td>\n","      <td>2020-02-01 03:00:00</td>\n","      <td>65.000000</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2020-02-01 04:00:00</td>\n","      <td>9417.6</td>\n","      <td>9441.9</td>\n","      <td>9391.9</td>\n","      <td>9420.98</td>\n","      <td>9391.9</td>\n","      <td>9393.514286</td>\n","      <td>9330.295238</td>\n","      <td>9358.037188</td>\n","      <td>9370.655540</td>\n","      <td>12.618352</td>\n","      <td>30.661128</td>\n","      <td>9391.617494</td>\n","      <td>9268.972982</td>\n","      <td>9400.434788</td>\n","      <td>9417.6</td>\n","      <td>9425.9</td>\n","      <td>9417.9</td>\n","      <td>9372.0</td>\n","      <td>9339.3</td>\n","      <td>0.997271</td>\n","      <td>0.996393</td>\n","      <td>0.997239</td>\n","      <td>1.002123</td>\n","      <td>1.005632</td>\n","      <td>2020-02-01 04:00:00</td>\n","      <td>61.000000</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1800</th>\n","      <td>2020-04-16 00:00:00</td>\n","      <td>6618.1</td>\n","      <td>6618.8</td>\n","      <td>6478.6</td>\n","      <td>6542.26</td>\n","      <td>6579.1</td>\n","      <td>6699.114286</td>\n","      <td>6767.261905</td>\n","      <td>6762.783889</td>\n","      <td>6708.852825</td>\n","      <td>-53.931064</td>\n","      <td>70.521638</td>\n","      <td>6908.305181</td>\n","      <td>6626.218629</td>\n","      <td>6604.147872</td>\n","      <td>6619.8</td>\n","      <td>6715.1</td>\n","      <td>6735.0</td>\n","      <td>6751.0</td>\n","      <td>6733.8</td>\n","      <td>0.993852</td>\n","      <td>0.979747</td>\n","      <td>0.976852</td>\n","      <td>0.974537</td>\n","      <td>0.977026</td>\n","      <td>2020-04-16 00:00:00</td>\n","      <td>2.845368</td>\n","      <td>0.104746</td>\n","      <td>0.113695</td>\n","      <td>0.133561</td>\n","      <td>0.119776</td>\n","    </tr>\n","    <tr>\n","      <th>1801</th>\n","      <td>2020-04-16 01:00:00</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.093476</td>\n","      <td>0.094227</td>\n","      <td>0.085303</td>\n","      <td>0.083091</td>\n","    </tr>\n","    <tr>\n","      <th>1802</th>\n","      <td>2020-04-16 02:00:00</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.130010</td>\n","      <td>0.143508</td>\n","      <td>0.161083</td>\n","      <td>0.146715</td>\n","    </tr>\n","    <tr>\n","      <th>1803</th>\n","      <td>2020-04-16 03:00:00</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.081475</td>\n","      <td>0.101235</td>\n","      <td>0.085270</td>\n","      <td>0.080011</td>\n","    </tr>\n","    <tr>\n","      <th>1804</th>\n","      <td>2020-04-16 04:00:00</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1805 rows × 31 columns</p>\n","</div>"],"text/plain":["               Unnamed: 0    Open    High     Low  Weighted Price   price  \\\n","0     2020-02-01 00:00:00  9337.1  9376.3  9336.0  9355.76         9372.0   \n","1     2020-02-01 01:00:00  9372.1  9418.0  9372.0  9396.57         9417.9   \n","2     2020-02-01 02:00:00  9418.0  9453.9  9396.3  9427.90         9425.9   \n","3     2020-02-01 03:00:00  9437.7  9447.7  9395.8  9412.27         9417.6   \n","4     2020-02-01 04:00:00  9417.6  9441.9  9391.9  9420.98         9391.9   \n","...                   ...     ...     ...     ...      ...            ...   \n","1800  2020-04-16 00:00:00  6618.1  6618.8  6478.6  6542.26         6579.1   \n","1801  2020-04-16 01:00:00 NaN     NaN     NaN     NaN             NaN       \n","1802  2020-04-16 02:00:00 NaN     NaN     NaN     NaN             NaN       \n","1803  2020-04-16 03:00:00 NaN     NaN     NaN     NaN             NaN       \n","1804  2020-04-16 04:00:00 NaN     NaN     NaN     NaN             NaN       \n","\n","              ma7         ma21        26ema        12ema       MACD  \\\n","0     9334.100000  9319.061905  9338.414300  9331.897840 -6.516460    \n","1     9351.100000  9320.085714  9344.302130  9345.128942  0.826812    \n","2     9371.285714  9324.890476  9350.346416  9357.555258  7.208842    \n","3     9387.600000  9328.014286  9355.328163  9366.792911  11.464748   \n","4     9393.514286  9330.295238  9358.037188  9370.655540  12.618352   \n","...           ...          ...          ...          ...        ...   \n","1800  6699.114286  6767.261905  6762.783889  6708.852825 -53.931064   \n","1801 NaN          NaN          NaN          NaN          NaN          \n","1802 NaN          NaN          NaN          NaN          NaN          \n","1803 NaN          NaN          NaN          NaN          NaN          \n","1804 NaN          NaN          NaN          NaN          NaN          \n","\n","           20sd   upper_band   lower_band          ema  momentum_1  \\\n","0     39.768078  9398.598060  9239.525749  9364.617840  9339.3       \n","1     46.991843  9414.069401  9226.102028  9400.139280  9372.0       \n","2     43.912695  9412.715866  9237.065087  9417.313093  9417.9       \n","3     34.745263  9397.504813  9258.523759  9417.504364  9425.9       \n","4     30.661128  9391.617494  9268.972982  9400.434788  9417.6       \n","...         ...          ...          ...          ...     ...       \n","1800  70.521638  6908.305181  6626.218629  6604.147872  6619.8       \n","1801 NaN        NaN          NaN          NaN          NaN           \n","1802 NaN        NaN          NaN          NaN          NaN           \n","1803 NaN        NaN          NaN          NaN          NaN           \n","1804 NaN        NaN          NaN          NaN          NaN           \n","\n","      momentum_2  momentum_3  momentum_4  momentum_5  momentum_1 return  \\\n","0     9390.0      9350.5      9303.4      9284.6      1.003501            \n","1     9339.3      9390.0      9350.5      9303.4      1.004898            \n","2     9372.0      9339.3      9390.0      9350.5      1.000849            \n","3     9417.9      9372.0      9339.3      9390.0      0.999119            \n","4     9425.9      9417.9      9372.0      9339.3      0.997271            \n","...      ...         ...         ...         ...           ...            \n","1800  6715.1      6735.0      6751.0      6733.8      0.993852            \n","1801 NaN         NaN         NaN         NaN         NaN                  \n","1802 NaN         NaN         NaN         NaN         NaN                  \n","1803 NaN         NaN         NaN         NaN         NaN                  \n","1804 NaN         NaN         NaN         NaN         NaN                  \n","\n","      momentum_2 return  momentum_3 return  momentum_4 return  \\\n","0     0.998083           1.002299           1.007374            \n","1     1.008416           1.002971           1.007208            \n","2     1.005751           1.009273           1.003823            \n","3     0.999968           1.004866           1.008384            \n","4     0.996393           0.997239           1.002123            \n","...        ...                ...                ...            \n","1800  0.979747           0.976852           0.974537            \n","1801 NaN                NaN                NaN                  \n","1802 NaN                NaN                NaN                  \n","1803 NaN                NaN                NaN                  \n","1804 NaN                NaN                NaN                  \n","\n","      momentum_5 return             datetime    bitcoin  Hourly Polarity Mean  \\\n","0     1.009413           2020-02-01 00:00:00  73.000000 NaN                     \n","1     1.012307           2020-02-01 01:00:00  67.000000 NaN                     \n","2     1.008064           2020-02-01 02:00:00  66.000000 NaN                     \n","3     1.002939           2020-02-01 03:00:00  65.000000 NaN                     \n","4     1.005632           2020-02-01 04:00:00  61.000000 NaN                     \n","...        ...                           ...        ...  ..                     \n","1800  0.977026           2020-04-16 00:00:00  2.845368   0.104746               \n","1801 NaN                 NaN                 NaN         0.093476               \n","1802 NaN                 NaN                 NaN         0.130010               \n","1803 NaN                 NaN                 NaN         0.081475               \n","1804 NaN                 NaN                 NaN         0.000000               \n","\n","      Hourly Polarity Weighted Mean 1  Hourly Polarity Weighted Mean 1_2  \\\n","0    NaN                              NaN                                  \n","1    NaN                              NaN                                  \n","2    NaN                              NaN                                  \n","3    NaN                              NaN                                  \n","4    NaN                              NaN                                  \n","...   ..                               ..                                  \n","1800  0.113695                         0.133561                            \n","1801  0.094227                         0.085303                            \n","1802  0.143508                         0.161083                            \n","1803  0.101235                         0.085270                            \n","1804  0.000000                         0.000000                            \n","\n","      Hourly Polarity Weighted Mean 1_4  \n","0    NaN                                 \n","1    NaN                                 \n","2    NaN                                 \n","3    NaN                                 \n","4    NaN                                 \n","...   ..                                 \n","1800  0.119776                           \n","1801  0.083091                           \n","1802  0.146715                           \n","1803  0.080011                           \n","1804  0.000000                           \n","\n","[1805 rows x 31 columns]"]},"metadata":{"tags":[]},"execution_count":54}]},{"cell_type":"code","metadata":{"id":"Vm2mZs6P-bNq","colab_type":"code","colab":{}},"source":["def load_data_single():\n","  dataset=pd.read_csv(\"dataset_search_sentiment.csv\")\n","  dataset.dropna(inplace=True)\n","  dataset['Open']=pd.to_numeric(dataset['Open'],downcast='float')\n","  dataset['High']=pd.to_numeric(dataset['High'],downcast='float')\n","  dataset['Low']=pd.to_numeric(dataset['Low'],downcast='float')\n","  dataset['Weighted Price']=pd.to_numeric(dataset['Weighted Price'],downcast='float')\n","\n","  dataset.replace([np.inf, -np.inf], np.nan,inplace=True)\n","  dataset.drop(columns='Unnamed: 0',inplace=True)\n","  #dataset.drop(columns='Timestamp',inplace=True)\n","  #dataset.drop(columns='datetime',inplace=True)\n","  dataset.drop(columns='datetime',inplace=True)\n","  #nn_features=dataset_multi_np[:,0:dataset_multi_np.shape[1]-dataset.shape[1]]\n","  nn_output=dataset.price[1:]\n","  nn_features=dataset[:dataset.shape[0]-1]\n","  nn_labels=np.divide(np.array(nn_output),np.array(dataset.price[:dataset.shape[0]-1]))>1\n","  #nn_feature_index=nn_features.dropna().index\n","  #nn_features_drop_nan=nn_features.dropna()\n","  #nn_labels_drop_nan=nn_labels[nn_feature_index]\n","\n","  #return nn_features_drop_nan,nn_labels_drop_nan\n","  return nn_features,nn_labels"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kCRoOmzFXd5i","colab_type":"code","colab":{}},"source":["def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n","\tn_vars = 1 if type(data) is list else data.shape[1]\n","\tdf = pd.DataFrame(data)\n","\tcols, names = list(), list()\n","\t# input sequence (t-n, ... t-1)\n","\tfor i in range(n_in, 0, -1):\n","\t\tcols.append(df.shift(i))\n","\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n","\t# forecast sequence (t, t+1, ... t+n)\n","\tfor i in range(0, n_out):\n","\t\tcols.append(df.shift(-i))\n","\t\tif i == 0:\n","\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n","\t\telse:\n","\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n","\t# put it all together\n","\tagg = pd.concat(cols, axis=1)\n","\tagg.columns = names\n","\t# drop rows with NaN values\n","\tif dropnan:\n","\t\tagg.dropna(inplace=True)\n","\treturn agg"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LT8IyidvDWge","colab_type":"code","colab":{}},"source":["def generate_rand_training(nn_features_scaled,nn_labels,num_training):\n","  #np.c_[nn_features_scaled, nn_labels]\n","  #select_array=np.random.choice(nn_features_scaled.shape[0], num_training, replace=False)\n","  concat_data=np.c_[nn_features_scaled, nn_labels]\n","  np.random.shuffle(concat_data)\n","  #print(concat_data)\n","  training, test = concat_data[:num_training,:], concat_data[num_training:,:]\n","  features_train = training[:,:nn_features_scaled.shape[1]]\n","  labels_train = training[:,-1]\n","  features_test =  test[:,:nn_features_scaled.shape[1]]\n","  labels_test = test[:,-1]\n","  #print(labels_train)\n","  #print(labels_test)\n","  return features_train,features_test,labels_train,labels_test"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yk-nO80cMojq","colab_type":"code","colab":{}},"source":["def generate_order_training(nn_features_scaled,nn_labels,num_training):\n","  features_train = nn_features_scaled[:num_training]\n","  features_test = nn_features_scaled[num_training:]\n","  labels_train = nn_labels[:num_training]\n","  labels_test = nn_labels[num_training:]\n","  #print(labels_train)\n","  #print(labels_test)\n","  return features_train,features_test,labels_train,labels_test"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VkSQiVuDXd6R","colab_type":"code","colab":{}},"source":["def fit_LSTM_multi(nn_features,nn_labels,num_steps,num_training):\n","\n","  from sklearn.preprocessing import StandardScaler\n","  sc = StandardScaler()\n","  nn_features_scaled = sc.fit_transform(nn_features)\n","  feature_num=int(nn_features.shape[1]/num_steps)\n","  #nn_features_scaled=nn_features_scaled.reshape((nn_features_scaled.shape[0],num_steps,feature_num))\n","  #nn_labels=nn_labels.reshape((nn_labels.shape[0],1,nn_labels.shape[1]))\n","  features_train,features_test,labels_train,labels_test=generate_rand_training(nn_features_scaled,nn_labels,num_training)\n","  features_train=features_train.reshape((features_train.shape[0],num_steps,feature_num))\n","  features_test=features_test.reshape((features_test.shape[0],num_steps,feature_num))\n","  #features_train = nn_features_scaled[:num_training]\n","  #features_test = nn_features_scaled[num_training:]\n","  #labels_train = nn_labels[:num_training]\n","  #labels_test = nn_labels[num_training:]\n","  #Dependencies\n","  import keras\n","  from keras.models import Sequential\n","  from keras.layers import Dense\n","  from keras.callbacks import History\n","  from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Bidirectional, Flatten, Dropout\n","  # Neural network\n","\n","  model = Sequential()\n","  model.add(LSTM(units=16, input_shape=(num_steps,feature_num)))\n","  model.add(Dropout(0.2))\n","  #model.add(LSTM(units=50, batch_input_shape=(8,1,23)))\n","  #model.add(Dropout(0.2))\n","  model.add(Dense(1, activation='sigmoid'))\n","  history = History()\n","  model.summary()\n","  model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n","\n","  from sklearn.model_selection import KFold\n","  results = []\n","  kf = KFold(n_splits=10)\n","  for train_idx, val_idx in kf.split(features_train, labels_train):\n","      X_train = features_train[train_idx]\n","      y_train = labels_train[train_idx]\n","      X_val = features_train[val_idx]\n","      y_val = labels_train[val_idx]\n","      hist = model.fit(X_train, y_train, batch_size = 8, epochs = 100, validation_data = (X_val, y_val), verbose = 1, callbacks=[history])\n","      results.append(hist.history)\n","\n","  print(results)\n","\n","  score = model.evaluate(features_test, labels_test,verbose=1)\n","  print(score)\n","  return hist"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"T9epmtOUUNId","colab_type":"code","colab":{}},"source":["def fit_NN_multi(nn_features,nn_labels,num_training):\n","  from sklearn.preprocessing import StandardScaler\n","  sc = StandardScaler()\n","  nn_features_scaled = sc.fit_transform(nn_features)\n","  #nn_labels=nn_labels.reshape((nn_labels.shape[0],1,nn_labels.shape[1]))\n","  features_train,features_test,labels_train,labels_test=generate_rand_training(nn_features_scaled,nn_labels,num_training)\n","  #print(features_train.shape)\n","  #print(features_test.shape)\n","  #print(labels_train.shape)\n","  #print(labels_test.shape)  \n","  import keras\n","  from keras.models import Sequential\n","  from keras.layers import Dense\n","  from keras.callbacks import History\n","  from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Bidirectional, Flatten, Dropout\n","  # Neural network\n","\n","  model = Sequential()\n","  model.add(Dense(16, input_dim=features_train.shape[1], activation='relu'))\n","  model.add(Dense(128, activation='relu'))\n","  model.add(Dense(1,activation='sigmoid'))\n","  history = History()\n","  \n","  print(model.summary())\n","\n","  model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])  \n","\n","  from sklearn.model_selection import KFold\n","  results = []\n","  kf = KFold(n_splits=5)\n","  for train_idx, val_idx in kf.split(features_train, labels_train):\n","      X_train = features_train[train_idx]\n","      y_train = labels_train[train_idx]\n","      X_val = features_train[val_idx]\n","      y_val = labels_train[val_idx]\n","      hist = model.fit(X_train, y_train, batch_size = 8, epochs = 100, validation_data = (X_val, y_val), verbose = 1, callbacks=[history])\n","      results.append(hist.history)\n","\n","  #print(results)\n","\n","  score = model.evaluate(features_test, labels_test,verbose=1)\n","  print(score)\n","  return score"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WyAtRUwIRkBo","colab_type":"code","colab":{}},"source":["def rebal_model(nn_features,nn_labels):\n","  results=[]\n","  rebal_days=180\n","  num_training=int(rebal_days/3*2)\n","  step_size=nn_features.shape[0]//rebal_days\n","  print(\"num_training: \",num_training)\n","  for i in range(step_size-1):\n","    print(\"running step: \",i)\n","    nn_features_step=nn_features[(i*rebal_days):(i*rebal_days+rebal_days),:]\n","    nn_labels_step=nn_labels[(i*rebal_days):(i*rebal_days+rebal_days)]\n","    #score=fit_NN_multi(nn_features_step,nn_labels_step,num_training)\n","    score=fit_rf(nn_features_step,nn_labels_step,num_training)\n","    results.append(score)\n","  return results"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BVmUFrEHJfPI","colab_type":"code","colab":{}},"source":["import sys\n","def rebal_model_consecutive(nn_features,nn_labels):\n","  results=[]\n","  rebal_window=168\n","  num_training=150\n","  #num_training=\n","  #step_size=nn_features.shape[0]//rebal_days\n","  print(\"num_training: \",num_training)\n","  for i in range(nn_features.shape[0]-rebal_window):\n","    print(\"running step: \",i)\n","    nn_features_step=nn_features[i:i+rebal_window,:]\n","    nn_labels_step=nn_labels[i:i+rebal_window]\n","    score=fit_rf(nn_features_step,nn_labels_step,num_training)\n","    #score=fit_NN_multi(nn_features_step,nn_labels_step,num_training)\n","    results.append(score)\n","    #sys.exit()\n","  return results"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TSYxHz-hjGyU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":72},"outputId":"ca95fc1d-b2a2-4afd-deb2-89d5e0f6e589","executionInfo":{"status":"ok","timestamp":1587060984581,"user_tz":240,"elapsed":184,"user":{"displayName":"Ziyun Chen","photoUrl":"","userId":"11569191328117568817"}}},"source":["import pandas as pd\n","import numpy as np\n","nn_features,nn_labels=load_data()"],"execution_count":28,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n","  app.launch_new_instance()\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"-A6yurAwfCDl","colab_type":"code","outputId":"01af5c5f-b4d7-4f4c-869b-761a191d196c","executionInfo":{"status":"error","timestamp":1587067249066,"user_tz":240,"elapsed":289,"user":{"displayName":"Ziyun Chen","photoUrl":"","userId":"11569191328117568817"}},"colab":{"base_uri":"https://localhost:8080/","height":388}},"source":["import pandas as pd\n","import numpy as np\n","nn_features,nn_labels=load_data_single()\n","#return_array=fit_NN_multi(nn_features,nn_labels,int(nn_features.shape[0]//4*3))\n","return_array=fit_rf(nn_features,nn_labels,int(nn_features.shape[0]//4*3))\n","print(return_array)\n","#return_array=rebal_model_consecutive(nn_features,nn_labels)"],"execution_count":52,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-52-352c28302e2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnn_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnn_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mload_data_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#return_array=fit_NN_multi(nn_features,nn_labels,int(nn_features.shape[0]//4*3))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mreturn_array\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_rf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnn_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#return_array=rebal_model_consecutive(nn_features,nn_labels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-44-54046563443a>\u001b[0m in \u001b[0;36mfit_rf\u001b[0;34m(nn_features, nn_labels, num_training)\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mnn_features_scaled_rf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0;31m#features_train,features_test,labels_train,labels_test=generate_rand_training(nn_features_scaled_rf,nn_labels,num_training)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mfeatures_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeatures_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerate_order_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_features_scaled_rf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnn_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_training\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;31m# Reset internal state before fitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    698\u001b[0m         X = check_array(X, accept_sparse=('csr', 'csc'),\n\u001b[1;32m    699\u001b[0m                         \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 700\u001b[0;31m                         force_all_finite='allow-nan')\n\u001b[0m\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;31m# Even in the case of `with_mean=False`, we update the mean anyway\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    529\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"unsafe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \"\"\"\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: could not convert string to float: '2020-02-09 08:00:00'"]}]},{"cell_type":"code","metadata":{"id":"NWL765WVPfAt","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"15986bde-0c0d-431b-c8cd-fdf4d6ad6820","executionInfo":{"status":"ok","timestamp":1587061589918,"user_tz":240,"elapsed":304,"user":{"displayName":"Ziyun Chen","photoUrl":"","userId":"11569191328117568817"}}},"source":["nn_features.shape"],"execution_count":40,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(928, 87)"]},"metadata":{"tags":[]},"execution_count":40}]},{"cell_type":"code","metadata":{"id":"LeB6-DrATd_T","colab_type":"code","outputId":"992a388f-336a-4cf5-e95b-9e83e394edb1","executionInfo":{"status":"ok","timestamp":1587061039613,"user_tz":240,"elapsed":35048,"user":{"displayName":"Ziyun Chen","photoUrl":"","userId":"11569191328117568817"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["import pandas as pd\n","import numpy as np\n","nn_features,nn_labels=load_data()\n","#num_steps=5\n","#return_array=fit_LSTM_multi(nn_features,nn_labels,num_steps,int(nn_features.shape[0]//5*4))\n","return_array=rebal_model_consecutive(nn_features,nn_labels)"],"execution_count":30,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n","  app.launch_new_instance()\n"],"name":"stderr"},{"output_type":"stream","text":["num_training:  150\n","running step:  0\n","running step:  1\n","running step:  2\n","running step:  3\n","running step:  4\n","running step:  5\n","running step:  6\n","running step:  7\n","running step:  8\n","running step:  9\n","running step:  10\n","running step:  11\n","running step:  12\n","running step:  13\n","running step:  14\n","running step:  15\n","running step:  16\n","running step:  17\n","running step:  18\n","running step:  19\n","running step:  20\n","running step:  21\n","running step:  22\n","running step:  23\n","running step:  24\n","running step:  25\n","running step:  26\n","running step:  27\n","running step:  28\n","running step:  29\n","running step:  30\n","running step:  31\n","running step:  32\n","running step:  33\n","running step:  34\n","running step:  35\n","running step:  36\n","running step:  37\n","running step:  38\n","running step:  39\n","running step:  40\n","running step:  41\n","running step:  42\n","running step:  43\n","running step:  44\n","running step:  45\n","running step:  46\n","running step:  47\n","running step:  48\n","running step:  49\n","running step:  50\n","running step:  51\n","running step:  52\n","running step:  53\n","running step:  54\n","running step:  55\n","running step:  56\n","running step:  57\n","running step:  58\n","running step:  59\n","running step:  60\n","running step:  61\n","running step:  62\n","running step:  63\n","running step:  64\n","running step:  65\n","running step:  66\n","running step:  67\n","running step:  68\n","running step:  69\n","running step:  70\n","running step:  71\n","running step:  72\n","running step:  73\n","running step:  74\n","running step:  75\n","running step:  76\n","running step:  77\n","running step:  78\n","running step:  79\n","running step:  80\n","running step:  81\n","running step:  82\n","running step:  83\n","running step:  84\n","running step:  85\n","running step:  86\n","running step:  87\n","running step:  88\n","running step:  89\n","running step:  90\n","running step:  91\n","running step:  92\n","running step:  93\n","running step:  94\n","running step:  95\n","running step:  96\n","running step:  97\n","running step:  98\n","running step:  99\n","running step:  100\n","running step:  101\n","running step:  102\n","running step:  103\n","running step:  104\n","running step:  105\n","running step:  106\n","running step:  107\n","running step:  108\n","running step:  109\n","running step:  110\n","running step:  111\n","running step:  112\n","running step:  113\n","running step:  114\n","running step:  115\n","running step:  116\n","running step:  117\n","running step:  118\n","running step:  119\n","running step:  120\n","running step:  121\n","running step:  122\n","running step:  123\n","running step:  124\n","running step:  125\n","running step:  126\n","running step:  127\n","running step:  128\n","running step:  129\n","running step:  130\n","running step:  131\n","running step:  132\n","running step:  133\n","running step:  134\n","running step:  135\n","running step:  136\n","running step:  137\n","running step:  138\n","running step:  139\n","running step:  140\n","running step:  141\n","running step:  142\n","running step:  143\n","running step:  144\n","running step:  145\n","running step:  146\n","running step:  147\n","running step:  148\n","running step:  149\n","running step:  150\n","running step:  151\n","running step:  152\n","running step:  153\n","running step:  154\n","running step:  155\n","running step:  156\n","running step:  157\n","running step:  158\n","running step:  159\n","running step:  160\n","running step:  161\n","running step:  162\n","running step:  163\n","running step:  164\n","running step:  165\n","running step:  166\n","running step:  167\n","running step:  168\n","running step:  169\n","running step:  170\n","running step:  171\n","running step:  172\n","running step:  173\n","running step:  174\n","running step:  175\n","running step:  176\n","running step:  177\n","running step:  178\n","running step:  179\n","running step:  180\n","running step:  181\n","running step:  182\n","running step:  183\n","running step:  184\n","running step:  185\n","running step:  186\n","running step:  187\n","running step:  188\n","running step:  189\n","running step:  190\n","running step:  191\n","running step:  192\n","running step:  193\n","running step:  194\n","running step:  195\n","running step:  196\n","running step:  197\n","running step:  198\n","running step:  199\n","running step:  200\n","running step:  201\n","running step:  202\n","running step:  203\n","running step:  204\n","running step:  205\n","running step:  206\n","running step:  207\n","running step:  208\n","running step:  209\n","running step:  210\n","running step:  211\n","running step:  212\n","running step:  213\n","running step:  214\n","running step:  215\n","running step:  216\n","running step:  217\n","running step:  218\n","running step:  219\n","running step:  220\n","running step:  221\n","running step:  222\n","running step:  223\n","running step:  224\n","running step:  225\n","running step:  226\n","running step:  227\n","running step:  228\n","running step:  229\n","running step:  230\n","running step:  231\n","running step:  232\n","running step:  233\n","running step:  234\n","running step:  235\n","running step:  236\n","running step:  237\n","running step:  238\n","running step:  239\n","running step:  240\n","running step:  241\n","running step:  242\n","running step:  243\n","running step:  244\n","running step:  245\n","running step:  246\n","running step:  247\n","running step:  248\n","running step:  249\n","running step:  250\n","running step:  251\n","running step:  252\n","running step:  253\n","running step:  254\n","running step:  255\n","running step:  256\n","running step:  257\n","running step:  258\n","running step:  259\n","running step:  260\n","running step:  261\n","running step:  262\n","running step:  263\n","running step:  264\n","running step:  265\n","running step:  266\n","running step:  267\n","running step:  268\n","running step:  269\n","running step:  270\n","running step:  271\n","running step:  272\n","running step:  273\n","running step:  274\n","running step:  275\n","running step:  276\n","running step:  277\n","running step:  278\n","running step:  279\n","running step:  280\n","running step:  281\n","running step:  282\n","running step:  283\n","running step:  284\n","running step:  285\n","running step:  286\n","running step:  287\n","running step:  288\n","running step:  289\n","running step:  290\n","running step:  291\n","running step:  292\n","running step:  293\n","running step:  294\n","running step:  295\n","running step:  296\n","running step:  297\n","running step:  298\n","running step:  299\n","running step:  300\n","running step:  301\n","running step:  302\n","running step:  303\n","running step:  304\n","running step:  305\n","running step:  306\n","running step:  307\n","running step:  308\n","running step:  309\n","running step:  310\n","running step:  311\n","running step:  312\n","running step:  313\n","running step:  314\n","running step:  315\n","running step:  316\n","running step:  317\n","running step:  318\n","running step:  319\n","running step:  320\n","running step:  321\n","running step:  322\n","running step:  323\n","running step:  324\n","running step:  325\n","running step:  326\n","running step:  327\n","running step:  328\n","running step:  329\n","running step:  330\n","running step:  331\n","running step:  332\n","running step:  333\n","running step:  334\n","running step:  335\n","running step:  336\n","running step:  337\n","running step:  338\n","running step:  339\n","running step:  340\n","running step:  341\n","running step:  342\n","running step:  343\n","running step:  344\n","running step:  345\n","running step:  346\n","running step:  347\n","running step:  348\n","running step:  349\n","running step:  350\n","running step:  351\n","running step:  352\n","running step:  353\n","running step:  354\n","running step:  355\n","running step:  356\n","running step:  357\n","running step:  358\n","running step:  359\n","running step:  360\n","running step:  361\n","running step:  362\n","running step:  363\n","running step:  364\n","running step:  365\n","running step:  366\n","running step:  367\n","running step:  368\n","running step:  369\n","running step:  370\n","running step:  371\n","running step:  372\n","running step:  373\n","running step:  374\n","running step:  375\n","running step:  376\n","running step:  377\n","running step:  378\n","running step:  379\n","running step:  380\n","running step:  381\n","running step:  382\n","running step:  383\n","running step:  384\n","running step:  385\n","running step:  386\n","running step:  387\n","running step:  388\n","running step:  389\n","running step:  390\n","running step:  391\n","running step:  392\n","running step:  393\n","running step:  394\n","running step:  395\n","running step:  396\n","running step:  397\n","running step:  398\n","running step:  399\n","running step:  400\n","running step:  401\n","running step:  402\n","running step:  403\n","running step:  404\n","running step:  405\n","running step:  406\n","running step:  407\n","running step:  408\n","running step:  409\n","running step:  410\n","running step:  411\n","running step:  412\n","running step:  413\n","running step:  414\n","running step:  415\n","running step:  416\n","running step:  417\n","running step:  418\n","running step:  419\n","running step:  420\n","running step:  421\n","running step:  422\n","running step:  423\n","running step:  424\n","running step:  425\n","running step:  426\n","running step:  427\n","running step:  428\n","running step:  429\n","running step:  430\n","running step:  431\n","running step:  432\n","running step:  433\n","running step:  434\n","running step:  435\n","running step:  436\n","running step:  437\n","running step:  438\n","running step:  439\n","running step:  440\n","running step:  441\n","running step:  442\n","running step:  443\n","running step:  444\n","running step:  445\n","running step:  446\n","running step:  447\n","running step:  448\n","running step:  449\n","running step:  450\n","running step:  451\n","running step:  452\n","running step:  453\n","running step:  454\n","running step:  455\n","running step:  456\n","running step:  457\n","running step:  458\n","running step:  459\n","running step:  460\n","running step:  461\n","running step:  462\n","running step:  463\n","running step:  464\n","running step:  465\n","running step:  466\n","running step:  467\n","running step:  468\n","running step:  469\n","running step:  470\n","running step:  471\n","running step:  472\n","running step:  473\n","running step:  474\n","running step:  475\n","running step:  476\n","running step:  477\n","running step:  478\n","running step:  479\n","running step:  480\n","running step:  481\n","running step:  482\n","running step:  483\n","running step:  484\n","running step:  485\n","running step:  486\n","running step:  487\n","running step:  488\n","running step:  489\n","running step:  490\n","running step:  491\n","running step:  492\n","running step:  493\n","running step:  494\n","running step:  495\n","running step:  496\n","running step:  497\n","running step:  498\n","running step:  499\n","running step:  500\n","running step:  501\n","running step:  502\n","running step:  503\n","running step:  504\n","running step:  505\n","running step:  506\n","running step:  507\n","running step:  508\n","running step:  509\n","running step:  510\n","running step:  511\n","running step:  512\n","running step:  513\n","running step:  514\n","running step:  515\n","running step:  516\n","running step:  517\n","running step:  518\n","running step:  519\n","running step:  520\n","running step:  521\n","running step:  522\n","running step:  523\n","running step:  524\n","running step:  525\n","running step:  526\n","running step:  527\n","running step:  528\n","running step:  529\n","running step:  530\n","running step:  531\n","running step:  532\n","running step:  533\n","running step:  534\n","running step:  535\n","running step:  536\n","running step:  537\n","running step:  538\n","running step:  539\n","running step:  540\n","running step:  541\n","running step:  542\n","running step:  543\n","running step:  544\n","running step:  545\n","running step:  546\n","running step:  547\n","running step:  548\n","running step:  549\n","running step:  550\n","running step:  551\n","running step:  552\n","running step:  553\n","running step:  554\n","running step:  555\n","running step:  556\n","running step:  557\n","running step:  558\n","running step:  559\n","running step:  560\n","running step:  561\n","running step:  562\n","running step:  563\n","running step:  564\n","running step:  565\n","running step:  566\n","running step:  567\n","running step:  568\n","running step:  569\n","running step:  570\n","running step:  571\n","running step:  572\n","running step:  573\n","running step:  574\n","running step:  575\n","running step:  576\n","running step:  577\n","running step:  578\n","running step:  579\n","running step:  580\n","running step:  581\n","running step:  582\n","running step:  583\n","running step:  584\n","running step:  585\n","running step:  586\n","running step:  587\n","running step:  588\n","running step:  589\n","running step:  590\n","running step:  591\n","running step:  592\n","running step:  593\n","running step:  594\n","running step:  595\n","running step:  596\n","running step:  597\n","running step:  598\n","running step:  599\n","running step:  600\n","running step:  601\n","running step:  602\n","running step:  603\n","running step:  604\n","running step:  605\n","running step:  606\n","running step:  607\n","running step:  608\n","running step:  609\n","running step:  610\n","running step:  611\n","running step:  612\n","running step:  613\n","running step:  614\n","running step:  615\n","running step:  616\n","running step:  617\n","running step:  618\n","running step:  619\n","running step:  620\n","running step:  621\n","running step:  622\n","running step:  623\n","running step:  624\n","running step:  625\n","running step:  626\n","running step:  627\n","running step:  628\n","running step:  629\n","running step:  630\n","running step:  631\n","running step:  632\n","running step:  633\n","running step:  634\n","running step:  635\n","running step:  636\n","running step:  637\n","running step:  638\n","running step:  639\n","running step:  640\n","running step:  641\n","running step:  642\n","running step:  643\n","running step:  644\n","running step:  645\n","running step:  646\n","running step:  647\n","running step:  648\n","running step:  649\n","running step:  650\n","running step:  651\n","running step:  652\n","running step:  653\n","running step:  654\n","running step:  655\n","running step:  656\n","running step:  657\n","running step:  658\n","running step:  659\n","running step:  660\n","running step:  661\n","running step:  662\n","running step:  663\n","running step:  664\n","running step:  665\n","running step:  666\n","running step:  667\n","running step:  668\n","running step:  669\n","running step:  670\n","running step:  671\n","running step:  672\n","running step:  673\n","running step:  674\n","running step:  675\n","running step:  676\n","running step:  677\n","running step:  678\n","running step:  679\n","running step:  680\n","running step:  681\n","running step:  682\n","running step:  683\n","running step:  684\n","running step:  685\n","running step:  686\n","running step:  687\n","running step:  688\n","running step:  689\n","running step:  690\n","running step:  691\n","running step:  692\n","running step:  693\n","running step:  694\n","running step:  695\n","running step:  696\n","running step:  697\n","running step:  698\n","running step:  699\n","running step:  700\n","running step:  701\n","running step:  702\n","running step:  703\n","running step:  704\n","running step:  705\n","running step:  706\n","running step:  707\n","running step:  708\n","running step:  709\n","running step:  710\n","running step:  711\n","running step:  712\n","running step:  713\n","running step:  714\n","running step:  715\n","running step:  716\n","running step:  717\n","running step:  718\n","running step:  719\n","running step:  720\n","running step:  721\n","running step:  722\n","running step:  723\n","running step:  724\n","running step:  725\n","running step:  726\n","running step:  727\n","running step:  728\n","running step:  729\n","running step:  730\n","running step:  731\n","running step:  732\n","running step:  733\n","running step:  734\n","running step:  735\n","running step:  736\n","running step:  737\n","running step:  738\n","running step:  739\n","running step:  740\n","running step:  741\n","running step:  742\n","running step:  743\n","running step:  744\n","running step:  745\n","running step:  746\n","running step:  747\n","running step:  748\n","running step:  749\n","running step:  750\n","running step:  751\n","running step:  752\n","running step:  753\n","running step:  754\n","running step:  755\n","running step:  756\n","running step:  757\n","running step:  758\n","running step:  759\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZPtZs1n7k5SK","colab_type":"code","outputId":"75378806-dfb7-4eda-a70e-d3fc4bc31bf2","executionInfo":{"status":"ok","timestamp":1587061050030,"user_tz":240,"elapsed":211,"user":{"displayName":"Ziyun Chen","photoUrl":"","userId":"11569191328117568817"}},"colab":{"base_uri":"https://localhost:8080/","height":72}},"source":["print(return_array)\n","roll_sum=0\n","for i in range(len(return_array)):\n","  roll_sum=roll_sum+return_array[i]\n","print(roll_sum/len(return_array))"],"execution_count":31,"outputs":[{"output_type":"stream","text":["[0.5555555555555556, 0.5555555555555556, 0.5, 0.3333333333333333, 0.5, 0.5555555555555556, 0.4444444444444444, 0.5, 0.4444444444444444, 0.5555555555555556, 0.6111111111111112, 0.5555555555555556, 0.6111111111111112, 0.6666666666666666, 0.6111111111111112, 0.6666666666666666, 0.5, 0.3333333333333333, 0.4444444444444444, 0.7222222222222222, 0.3888888888888889, 0.5555555555555556, 0.7222222222222222, 0.5, 0.5, 0.5555555555555556, 0.6666666666666666, 0.5555555555555556, 0.5, 0.6666666666666666, 0.6111111111111112, 0.6666666666666666, 0.5, 0.6111111111111112, 0.5555555555555556, 0.7777777777777778, 0.6666666666666666, 0.6666666666666666, 0.5, 0.7222222222222222, 0.5, 0.4444444444444444, 0.5555555555555556, 0.5555555555555556, 0.3888888888888889, 0.3333333333333333, 0.5, 0.6111111111111112, 0.3888888888888889, 0.3888888888888889, 0.4444444444444444, 0.2777777777777778, 0.7777777777777778, 0.4444444444444444, 0.6111111111111112, 0.3333333333333333, 0.5, 0.5, 0.5, 0.5, 0.3333333333333333, 0.3888888888888889, 0.7222222222222222, 0.5, 0.5, 0.6111111111111112, 0.3333333333333333, 0.3888888888888889, 0.6666666666666666, 0.5555555555555556, 0.5555555555555556, 0.5, 0.4444444444444444, 0.7222222222222222, 0.7222222222222222, 0.6666666666666666, 0.7222222222222222, 0.5555555555555556, 0.5555555555555556, 0.3888888888888889, 0.6666666666666666, 0.5555555555555556, 0.6111111111111112, 0.5555555555555556, 0.3888888888888889, 0.5555555555555556, 0.5555555555555556, 0.4444444444444444, 0.6111111111111112, 0.6666666666666666, 0.4444444444444444, 0.4444444444444444, 0.3333333333333333, 0.2222222222222222, 0.5, 0.4444444444444444, 0.6111111111111112, 0.3888888888888889, 0.5555555555555556, 0.3888888888888889, 0.4444444444444444, 0.3888888888888889, 0.3888888888888889, 0.5555555555555556, 0.4444444444444444, 0.4444444444444444, 0.3333333333333333, 0.5, 0.4444444444444444, 0.6111111111111112, 0.5555555555555556, 0.3888888888888889, 0.5, 0.6111111111111112, 0.5, 0.6666666666666666, 0.2777777777777778, 0.6111111111111112, 0.4444444444444444, 0.5, 0.5555555555555556, 0.5555555555555556, 0.3888888888888889, 0.5, 0.3333333333333333, 0.6666666666666666, 0.4444444444444444, 0.4444444444444444, 0.2777777777777778, 0.5, 0.4444444444444444, 0.6111111111111112, 0.5, 0.4444444444444444, 0.6111111111111112, 0.3888888888888889, 0.7222222222222222, 0.4444444444444444, 0.6111111111111112, 0.5555555555555556, 0.5555555555555556, 0.5, 0.6666666666666666, 0.4444444444444444, 0.6111111111111112, 0.5555555555555556, 0.2777777777777778, 0.7222222222222222, 0.6111111111111112, 0.5, 0.6111111111111112, 0.7222222222222222, 0.4444444444444444, 0.4444444444444444, 0.3333333333333333, 0.4444444444444444, 0.5, 0.5555555555555556, 0.3333333333333333, 0.4444444444444444, 0.3888888888888889, 0.3888888888888889, 0.3888888888888889, 0.6111111111111112, 0.4444444444444444, 0.5555555555555556, 0.6111111111111112, 0.5, 0.6111111111111112, 0.5, 0.5555555555555556, 0.5, 0.4444444444444444, 0.6111111111111112, 0.6666666666666666, 0.6111111111111112, 0.4444444444444444, 0.3888888888888889, 0.3333333333333333, 0.4444444444444444, 0.3888888888888889, 0.5555555555555556, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3888888888888889, 0.2222222222222222, 0.3888888888888889, 0.2777777777777778, 0.2777777777777778, 0.3333333333333333, 0.2777777777777778, 0.3888888888888889, 0.4444444444444444, 0.3888888888888889, 0.2222222222222222, 0.3333333333333333, 0.3888888888888889, 0.3888888888888889, 0.6666666666666666, 0.5, 0.4444444444444444, 0.5555555555555556, 0.5555555555555556, 0.7222222222222222, 0.6111111111111112, 0.6111111111111112, 0.5, 0.4444444444444444, 0.4444444444444444, 0.3888888888888889, 0.3888888888888889, 0.6111111111111112, 0.5, 0.5, 0.4444444444444444, 0.6111111111111112, 0.4444444444444444, 0.5555555555555556, 0.5, 0.3333333333333333, 0.5, 0.3888888888888889, 0.3888888888888889, 0.3888888888888889, 0.4444444444444444, 0.2777777777777778, 0.3333333333333333, 0.3333333333333333, 0.4444444444444444, 0.3888888888888889, 0.5, 0.5, 0.6111111111111112, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.4444444444444444, 0.5, 0.5555555555555556, 0.3333333333333333, 0.4444444444444444, 0.4444444444444444, 0.5, 0.6666666666666666, 0.2777777777777778, 0.4444444444444444, 0.4444444444444444, 0.3333333333333333, 0.6111111111111112, 0.5555555555555556, 0.3888888888888889, 0.3333333333333333, 0.5555555555555556, 0.5, 0.5, 0.3888888888888889, 0.5, 0.5, 0.4444444444444444, 0.7222222222222222, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.6666666666666666, 0.7777777777777778, 0.5555555555555556, 0.3888888888888889, 0.4444444444444444, 0.5, 0.5555555555555556, 0.4444444444444444, 0.6111111111111112, 0.6111111111111112, 0.5, 0.4444444444444444, 0.3333333333333333, 0.4444444444444444, 0.3333333333333333, 0.5, 0.3888888888888889, 0.3333333333333333, 0.3333333333333333, 0.6111111111111112, 0.4444444444444444, 0.4444444444444444, 0.5, 0.4444444444444444, 0.3333333333333333, 0.5, 0.5, 0.5555555555555556, 0.5555555555555556, 0.4444444444444444, 0.3888888888888889, 0.5, 0.6111111111111112, 0.4444444444444444, 0.5555555555555556, 0.3888888888888889, 0.2777777777777778, 0.5, 0.3888888888888889, 0.3888888888888889, 0.5, 0.4444444444444444, 0.6111111111111112, 0.6111111111111112, 0.7777777777777778, 0.3888888888888889, 0.3888888888888889, 0.5, 0.3888888888888889, 0.3888888888888889, 0.4444444444444444, 0.3333333333333333, 0.2777777777777778, 0.2222222222222222, 0.5555555555555556, 0.5, 0.2777777777777778, 0.5, 0.5, 0.3333333333333333, 0.3333333333333333, 0.3888888888888889, 0.2777777777777778, 0.3888888888888889, 0.3888888888888889, 0.3888888888888889, 0.4444444444444444, 0.3333333333333333, 0.3333333333333333, 0.6666666666666666, 0.6111111111111112, 0.4444444444444444, 0.5555555555555556, 0.7777777777777778, 0.3888888888888889, 0.5, 0.6666666666666666, 0.7222222222222222, 0.6111111111111112, 0.4444444444444444, 0.7222222222222222, 0.6111111111111112, 0.6111111111111112, 0.5555555555555556, 0.6111111111111112, 0.8333333333333334, 0.5555555555555556, 0.3333333333333333, 0.5, 0.4444444444444444, 0.4444444444444444, 0.3888888888888889, 0.4444444444444444, 0.5555555555555556, 0.5, 0.5555555555555556, 0.5, 0.5555555555555556, 0.4444444444444444, 0.6111111111111112, 0.6111111111111112, 0.3888888888888889, 0.3888888888888889, 0.5, 0.5555555555555556, 0.6666666666666666, 0.3888888888888889, 0.4444444444444444, 0.5, 0.3888888888888889, 0.2222222222222222, 0.3333333333333333, 0.4444444444444444, 0.4444444444444444, 0.3333333333333333, 0.3888888888888889, 0.3888888888888889, 0.2222222222222222, 0.3888888888888889, 0.2777777777777778, 0.3888888888888889, 0.3333333333333333, 0.3888888888888889, 0.6111111111111112, 0.6111111111111112, 0.5555555555555556, 0.5, 0.5555555555555556, 0.5555555555555556, 0.6666666666666666, 0.6111111111111112, 0.5555555555555556, 0.5, 0.5555555555555556, 0.6666666666666666, 0.5555555555555556, 0.7777777777777778, 0.7222222222222222, 0.5, 0.5555555555555556, 0.5, 0.4444444444444444, 0.6111111111111112, 0.3888888888888889, 0.3888888888888889, 0.5555555555555556, 0.5, 0.6666666666666666, 0.5555555555555556, 0.5555555555555556, 0.7777777777777778, 0.4444444444444444, 0.6666666666666666, 0.5, 0.5, 0.5555555555555556, 0.3888888888888889, 0.3333333333333333, 0.5, 0.4444444444444444, 0.6666666666666666, 0.5555555555555556, 0.3888888888888889, 0.3888888888888889, 0.5, 0.4444444444444444, 0.4444444444444444, 0.3888888888888889, 0.4444444444444444, 0.2777777777777778, 0.3333333333333333, 0.5, 0.3333333333333333, 0.6111111111111112, 0.5555555555555556, 0.5555555555555556, 0.6666666666666666, 0.5, 0.7222222222222222, 0.6111111111111112, 0.5555555555555556, 0.5555555555555556, 0.4444444444444444, 0.5555555555555556, 0.5555555555555556, 0.6666666666666666, 0.5555555555555556, 0.5, 0.6111111111111112, 0.5, 0.4444444444444444, 0.5, 0.5555555555555556, 0.4444444444444444, 0.6666666666666666, 0.4444444444444444, 0.5, 0.5555555555555556, 0.5, 0.3888888888888889, 0.5, 0.5555555555555556, 0.5, 0.6666666666666666, 0.5555555555555556, 0.5555555555555556, 0.7777777777777778, 0.6111111111111112, 0.7222222222222222, 0.5555555555555556, 0.6111111111111112, 0.5, 0.4444444444444444, 0.5555555555555556, 0.4444444444444444, 0.5, 0.5, 0.5555555555555556, 0.6111111111111112, 0.4444444444444444, 0.3888888888888889, 0.4444444444444444, 0.3888888888888889, 0.5555555555555556, 0.4444444444444444, 0.6111111111111112, 0.6111111111111112, 0.5555555555555556, 0.5555555555555556, 0.6666666666666666, 0.6666666666666666, 0.6111111111111112, 0.6666666666666666, 0.8333333333333334, 0.5555555555555556, 0.6666666666666666, 0.6666666666666666, 0.6111111111111112, 0.4444444444444444, 0.6666666666666666, 0.4444444444444444, 0.6111111111111112, 0.3888888888888889, 0.7222222222222222, 0.6111111111111112, 0.5, 0.4444444444444444, 0.5, 0.4444444444444444, 0.3333333333333333, 0.3888888888888889, 0.4444444444444444, 0.6666666666666666, 0.3888888888888889, 0.5555555555555556, 0.5555555555555556, 0.4444444444444444, 0.3888888888888889, 0.2777777777777778, 0.4444444444444444, 0.3888888888888889, 0.3333333333333333, 0.5555555555555556, 0.6666666666666666, 0.5, 0.6111111111111112, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6111111111111112, 0.6666666666666666, 0.5, 0.5555555555555556, 0.6111111111111112, 0.6666666666666666, 0.5, 0.6666666666666666, 0.5555555555555556, 0.6666666666666666, 0.6111111111111112, 0.5, 0.4444444444444444, 0.6111111111111112, 0.5555555555555556, 0.6111111111111112, 0.5, 0.5, 0.5, 0.5, 0.4444444444444444, 0.5, 0.5, 0.5555555555555556, 0.5555555555555556, 0.4444444444444444, 0.6111111111111112, 0.5555555555555556, 0.5, 0.5555555555555556, 0.4444444444444444, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5, 0.5555555555555556, 0.6666666666666666, 0.6111111111111112, 0.5555555555555556, 0.5555555555555556, 0.6666666666666666, 0.6111111111111112, 0.5, 0.3888888888888889, 0.6111111111111112, 0.4444444444444444, 0.4444444444444444, 0.2777777777777778, 0.3333333333333333, 0.5555555555555556, 0.3333333333333333, 0.4444444444444444, 0.4444444444444444, 0.4444444444444444, 0.4444444444444444, 0.3333333333333333, 0.4444444444444444, 0.3888888888888889, 0.3333333333333333, 0.4444444444444444, 0.5555555555555556, 0.6111111111111112, 0.6111111111111112, 0.6666666666666666, 0.5, 0.3888888888888889, 0.6666666666666666, 0.5555555555555556, 0.5, 0.5555555555555556, 0.5, 0.6666666666666666, 0.6111111111111112, 0.3888888888888889, 0.5555555555555556, 0.4444444444444444, 0.3888888888888889, 0.3333333333333333, 0.3888888888888889, 0.5, 0.5, 0.3888888888888889, 0.6111111111111112, 0.5, 0.4444444444444444, 0.2777777777777778, 0.5, 0.4444444444444444, 0.3333333333333333, 0.4444444444444444, 0.3333333333333333, 0.3333333333333333, 0.3888888888888889, 0.3333333333333333, 0.3888888888888889, 0.3888888888888889, 0.5, 0.3888888888888889, 0.4444444444444444, 0.5, 0.7777777777777778, 0.3888888888888889, 0.5555555555555556, 0.5, 0.3333333333333333, 0.4444444444444444, 0.3888888888888889, 0.3888888888888889, 0.5, 0.4444444444444444, 0.5, 0.4444444444444444, 0.2777777777777778, 0.5555555555555556, 0.3888888888888889, 0.4444444444444444, 0.5, 0.6111111111111112, 0.5555555555555556, 0.5, 0.4444444444444444, 0.5, 0.5, 0.5555555555555556, 0.6111111111111112, 0.6111111111111112, 0.4444444444444444, 0.3888888888888889, 0.6111111111111112, 0.6666666666666666, 0.5555555555555556, 0.3333333333333333, 0.4444444444444444, 0.3888888888888889, 0.6111111111111112, 0.3888888888888889, 0.2777777777777778, 0.3333333333333333, 0.3888888888888889, 0.3888888888888889, 0.3333333333333333, 0.2222222222222222, 0.3888888888888889, 0.2777777777777778, 0.2222222222222222, 0.2777777777777778, 0.2777777777777778, 0.3333333333333333, 0.2777777777777778, 0.3888888888888889, 0.3333333333333333, 0.4444444444444444, 0.5, 0.4444444444444444, 0.4444444444444444, 0.16666666666666666, 0.4444444444444444, 0.5555555555555556, 0.5, 0.3333333333333333, 0.5, 0.4444444444444444, 0.3333333333333333, 0.4444444444444444, 0.5555555555555556, 0.5, 0.5555555555555556, 0.6111111111111112, 0.5, 0.5555555555555556, 0.2777777777777778, 0.5, 0.3888888888888889, 0.5555555555555556, 0.4444444444444444, 0.5, 0.3888888888888889, 0.3333333333333333, 0.3333333333333333, 0.5555555555555556, 0.3333333333333333, 0.5555555555555556, 0.3333333333333333, 0.3333333333333333, 0.2777777777777778, 0.2222222222222222, 0.2222222222222222, 0.5, 0.4444444444444444, 0.3888888888888889, 0.5, 0.4444444444444444, 0.5555555555555556, 0.6666666666666666, 0.6111111111111112, 0.7777777777777778, 0.3333333333333333, 0.5555555555555556, 0.7222222222222222, 0.6111111111111112, 0.7222222222222222, 0.4444444444444444, 0.6111111111111112, 0.5555555555555556, 0.6666666666666666, 0.3888888888888889, 0.7222222222222222, 0.6111111111111112, 0.5555555555555556, 0.6111111111111112, 0.6666666666666666, 0.5555555555555556, 0.5555555555555556, 0.6111111111111112, 0.5555555555555556, 0.4444444444444444, 0.3333333333333333, 0.4444444444444444, 0.4444444444444444, 0.5555555555555556, 0.5, 0.6111111111111112, 0.6111111111111112, 0.4444444444444444, 0.6111111111111112, 0.2777777777777778, 0.3888888888888889]\n","0.4937865497076019\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fbtuGnhoybSb","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"a2036e24-e55e-4cfb-b7cd-9a00f43b0377","executionInfo":{"status":"ok","timestamp":1586997083096,"user_tz":240,"elapsed":229,"user":{"displayName":"Ziyun Chen","photoUrl":"","userId":"11569191328117568817"}}},"source":["np.mean(return_array)"],"execution_count":60,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.5568980667838312"]},"metadata":{"tags":[]},"execution_count":60}]},{"cell_type":"code","metadata":{"id":"-mlsDIwVXd67","colab_type":"code","colab":{}},"source":["def fit_rf(nn_features,nn_labels,num_training):\n","  from sklearn.preprocessing import StandardScaler\n","  sc = StandardScaler()\n","  nn_features_scaled_rf = sc.fit_transform(nn_features)\n","  #features_train,features_test,labels_train,labels_test=generate_rand_training(nn_features_scaled_rf,nn_labels,num_training)\n","  features_train,features_test,labels_train,labels_test=generate_order_training(nn_features_scaled_rf,nn_labels,num_training)\n","  # Import the model we are using\n","  from sklearn.ensemble import RandomForestRegressor\n","  # Instantiate model with 1000 decision trees\n","  rf = RandomForestRegressor(n_estimators = 200, random_state = 42)\n","  # Train the model on training data\n","  rf.fit(features_train, labels_train);\n","  predictions = rf.predict(features_test)\n","  prediction_dummy=predictions>0.5\n","  #score=rf.score(predictions,labels_test)\n","  # Calculate the absolute errors\n","  #errors = abs(predictions - labels_test)\n","  # Print out the mean absolute error (mae)\n","  #print('Mean Absolute Error:', round(np.mean(errors), 2))\n","  accuracy_score=np.equal(prediction_dummy,labels_test).sum()/prediction_dummy.shape[0]\n","  #print('prediction accuracy',accuracy_score)\n","  return accuracy_score"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iBviadICX9dJ","colab_type":"code","outputId":"fa51710a-7fc4-400c-f427-c546fea28f3d","executionInfo":{"status":"ok","timestamp":1587061654316,"user_tz":240,"elapsed":4655,"user":{"displayName":"Ziyun Chen","photoUrl":"","userId":"11569191328117568817"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["fit_rf(nn_features,nn_labels,600)"],"execution_count":46,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.4847560975609756"]},"metadata":{"tags":[]},"execution_count":46}]},{"cell_type":"code","metadata":{"id":"q1vdSIICulOe","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":87},"outputId":"8cf7e1ff-e2fc-4b6e-bd7c-fa11ac6394fd","executionInfo":{"status":"ok","timestamp":1586969084216,"user_tz":240,"elapsed":2218,"user":{"displayName":"Ziyun Chen","photoUrl":"","userId":"11569191328117568817"}}},"source":["return_array=rebal_model(nn_features,nn_labels)"],"execution_count":29,"outputs":[{"output_type":"stream","text":["num_training:  120\n","running step:  0\n","running step:  1\n","running step:  2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ppcDVqgyuqId","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":240},"outputId":"f10c4394-6fe7-483d-99dc-e033cbd83c2e","executionInfo":{"status":"error","timestamp":1586969094958,"user_tz":240,"elapsed":396,"user":{"displayName":"Ziyun Chen","photoUrl":"","userId":"11569191328117568817"}}},"source":["print(return_array)\n","roll_sum=0\n","for i in range(len(return_array)):\n","  roll_sum=roll_sum+return_array[i][1]\n","print(roll_sum/len(return_array))"],"execution_count":30,"outputs":[{"output_type":"stream","text":["[0.6666666666666666, 0.5166666666666667, 0.6333333333333333]\n"],"name":"stdout"},{"output_type":"error","ename":"IndexError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-30-8c4c62d00b3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mroll_sum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mroll_sum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mroll_sum\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mreturn_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroll_sum\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: invalid index to scalar variable."]}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"xwNpbsiGyNrL","colab":{}},"source":["X_test_orig=nn_features[1000:]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"6-d3D2Snx-p1","colab":{}},"source":["def plot_loss(hist):\n","  import matplotlib.pyplot as plt\n","  plt.plot(hist.history['loss'])\n","  plt.plot(hist.history['val_loss'])\n","  plt.title('Model loss')\n","  plt.ylabel('Loss')\n","  plt.xlabel('Epoch')\n","  plt.legend(['Train', 'Val'], loc='upper right')\n","  plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"_6dDmYX-vmKM","outputId":"ba7d0340-25c9-450a-8047-122fd035eabc","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["predictions.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(390,)"]},"metadata":{"tags":[]},"execution_count":120}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"4NwpNaUspzGj","outputId":"e6057802-f904-4167-9007-df041d7d052a","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["print (get_accuracy(X_test_orig,y_test,predictions))"],"execution_count":0,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'get_accuracy' is not defined","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32m<ipython-input-121-8f3a247ad4f6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mget_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_orig\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[1;31mNameError\u001b[0m: name 'get_accuracy' is not defined"]}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"dTU7iWZ9y8xY","outputId":"294ce233-b1c1-4ec9-83fe-1a338ae69614","colab":{}},"source":["\n","# Import tools needed for visualization\n","from sklearn.tree import export_graphviz\n","#import pydot\n","# Pull out one tree from the forest\n","tree = rf.estimators_[5]\n","# Export the image to a dot file\n","feature_list = list(dataset.columns)\n","export_graphviz(tree, out_file = 'tree.dot', feature_names = feature_list, rounded = True, precision = 1)\n","# Use dot file to create a graph\n","(graph, ) = pydot.graph_from_dot_file('tree.dot')\n","# Write graph to a png file\n","graph.write_png('tree.png')"],"execution_count":0,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'pydot'","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[1;32m<ipython-input-68-b590d0d3905c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# Import tools needed for visualization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mexport_graphviz\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpydot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;31m# Pull out one tree from the forest\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mtree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pydot'"]}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"OURmlIRTzN3r","outputId":"908fc2a6-1e50-4ab5-86b7-02965c1fc9cc","colab":{"base_uri":"https://localhost:8080/","height":390}},"source":["feature_list"],"execution_count":0,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'feature_list' is not defined","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32m<ipython-input-67-8c457ebfb11f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfeature_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[1;31mNameError\u001b[0m: name 'feature_list' is not defined"]}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"KAdzoa1Ozv5E","colab":{}},"source":["files.download('tree.png') "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2a5FyY5kexpr","colab_type":"code","colab":{}},"source":["num_traing=600"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"P7lOPlV718vQ","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"b693991a-d93c-4d2a-d441-347d22a4b751","executionInfo":{"status":"ok","timestamp":1586998503155,"user_tz":240,"elapsed":642,"user":{"displayName":"Ziyun Chen","photoUrl":"","userId":"11569191328117568817"}}},"source":["from sklearn.ensemble import AdaBoostClassifier\n","model = AdaBoostClassifier(random_state=4)\n","model.fit(nn_features[:num_traing], nn_labels[:num_traing])\n","model.score(nn_features[num_traing:],nn_labels[num_traing:])"],"execution_count":92,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.5547445255474452"]},"metadata":{"tags":[]},"execution_count":92}]},{"cell_type":"code","metadata":{"id":"dKB88AEpd0PB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"939baa38-19e3-4a9b-87f1-b1cbaa21c142","executionInfo":{"status":"ok","timestamp":1586998504402,"user_tz":240,"elapsed":257,"user":{"displayName":"Ziyun Chen","photoUrl":"","userId":"11569191328117568817"}}},"source":["nn_features[:600].shape"],"execution_count":93,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(600, 90)"]},"metadata":{"tags":[]},"execution_count":93}]},{"cell_type":"code","metadata":{"id":"cs8iXLC2d2E4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"d81774ca-3018-4759-ff71-d7adc904b3c6","executionInfo":{"status":"ok","timestamp":1586998508747,"user_tz":240,"elapsed":261,"user":{"displayName":"Ziyun Chen","photoUrl":"","userId":"11569191328117568817"}}},"source":["nn_features[600:].shape"],"execution_count":94,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(137, 90)"]},"metadata":{"tags":[]},"execution_count":94}]},{"cell_type":"code","metadata":{"id":"hmrJtpBMd5VU","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}